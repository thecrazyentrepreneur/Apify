# Actor File Structure

```
influencer-data-scraper/
â”‚
â”œâ”€â”€ .actor/
â”‚   â”œâ”€â”€ actor.json              # Actor configuration & metadata
â”‚   â””â”€â”€ input_schema.json       # Input form schema (UI in Apify Console)
â”‚
â”œâ”€â”€ src/
â”‚   â””â”€â”€ main.js                 # Main scraping logic
â”‚
â”œâ”€â”€ package.json                # NPM dependencies
â”œâ”€â”€ Dockerfile                  # Container configuration
â”œâ”€â”€ README.md                   # Full documentation
â”œâ”€â”€ DEPLOYMENT.md              # Deployment guide
â””â”€â”€ input.json                 # Sample input for testing

```

## File Descriptions

### `.actor/actor.json`
- Defines actor name, title, description
- Configures input/output storage
- Sets display properties for results

### `.actor/input_schema.json`
- Creates the input form in Apify Console
- Defines fields: platformLinks, executive, team, etc.
- Sets validation rules and examples

### `src/main.js`
- **Main scraping engine**
- Contains scrapers for Instagram, TikTok, YouTube
- Extracts: followers, views, engagement rate
- Formats output to match your spreadsheet

### `package.json`
- Lists required NPM packages
- Apify SDK, Crawlee, Puppeteer
- Defines start script

### `Dockerfile`
- Specifies Node.js + Puppeteer environment
- Based on official Apify image
- Installs dependencies and copies code

### `README.md`
- Complete user documentation
- Features, usage examples, troubleshooting
- Export and scheduling instructions

### `DEPLOYMENT.md`
- Step-by-step deployment guide
- Multiple deployment options
- Google Sheets integration

### `input.json`
- Sample input for local testing
- Shows correct JSON format
- Can be used as template

---

## How Data Flows

```
1. User Input (URLs + metadata)
        â†“
2. Apify Actor Initialization
        â†“
3. Puppeteer Browser Launch
        â†“
4. For each URL:
   - Detect platform (IG/TikTok/YouTube)
   - Navigate to profile
   - Extract data (followers, views, etc.)
   - Calculate engagement rate
   - Add metadata (date, team, executive)
        â†“
5. Combine all results
        â†“
6. Save to Apify Dataset
        â†“
7. Export to CSV/Sheets/JSON
```

---

## Customization Points

### Add New Platform
Edit `src/main.js` â†’ Add new scraper function:
```javascript
async function scrapeLinkedIn(page, url, log) {
  // Your scraping logic here
}
```

### Change Output Format
Edit `src/main.js` â†’ Modify result object:
```javascript
const result = {
  'Custom Field': 'value',
  // ... rest of fields
};
```

### Add More Input Fields
Edit `.actor/input_schema.json` â†’ Add property:
```json
"newField": {
  "title": "New Field",
  "type": "string",
  "editor": "textfield"
}
```

### Adjust Timeouts
Edit `src/main.js` â†’ Change timeout values:
```javascript
navigationTimeoutSecs: 60,  // Increase if needed
```

---

## Next Steps

1. âœ… Files created and ready
2. ğŸ“¤ Upload to Apify Console
3. ğŸ”¨ Build the actor
4. â–¶ï¸ Run test with sample URLs
5. ğŸ“Š Export to your spreadsheet
6. â° Set up weekly schedule
7. ğŸ¯ Start collecting creator data!

---

**All files are ready for deployment!**
